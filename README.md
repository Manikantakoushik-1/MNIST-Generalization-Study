# MNIST Generalization Study  
### Overfitting vs Regularization in Deep Learning (PyTorch)

---

> A focused study on generalization in deep learning

> Demonstrating overfitting and regularization dynamics on MNIST.


## ðŸ“Œ Overview

This project explores **model generalization** by intentionally:

- Creating an overfitting model
- Applying regularization techniques
- Comparing training vs validation behavior

The goal is to understand how deep networks memorize vs generalize.

---

## ðŸŽ¯ Objectives

1. Build a strong baseline CNN
2. Force overfitting on MNIST
3. Reduce generalization gap using regularization
4. Analyze training dynamics

---

## ðŸ§  Key Learnings

- High-capacity models easily memorize data
- Overfitting appears as a growing trainâ€“val gap
- Regularization improves generalization
- Data augmentation acts as implicit regularization

---

## ðŸ›  Tech Stack

- PyTorch
- Torchvision
- NumPy
- Matplotlib

---

## ðŸ“‚ Project Structure

src/ â†’ training scripts

results/ â†’ plots and logs

notebooks/ â†’ experiments


---

## ðŸ“Š Experiments

### Task 1 â€” Baseline CNN
- Standard CNN
- Good generalization
- Small trainâ€“val gap

### Task 2 â€” Overfitting
- Large MLP
- No dropout
- High capacity
- Gap > 2%

### Task 3 â€” Regularization
Applied:

- Dropout
- Weight decay
- Label smoothing
- Data augmentation
- LR scheduling

Result:

âœ… Gap < 0.5%  
âœ… Stable validation accuracy

---

## ðŸ“ˆ Results

### Overfitting Curve
![Overfitting](results/task2_curves.png)

### Regularized Curve
![Regularized](results/task3_curves.png)

---

## ðŸš€ How to Run

pip install -r requirements.txt 

python src/task1_baseline.py

python src/task2_overfit.py

python src/task3_regularized.py


---

## ðŸ“Œ Future Work

- Try CIFAR-10
- Explore Mixup/CutMix
- Study double descent

---

## ðŸ‘¤ Author

Manikanta Koushik  
AI/ML Student & Research Intern
